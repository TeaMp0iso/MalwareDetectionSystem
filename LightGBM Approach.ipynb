{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Fix  Memory Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping monkeypatch for pd.DataFrame.__del__: libc or malloc_trim() not found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, sys\n",
    "from ctypes import cdll, CDLL\n",
    "try:\n",
    "    cdll.LoadLibrary(\"libc.so.6\")\n",
    "    libc = CDLL(\"libc.so.6\")\n",
    "    libc.malloc_trim(0)\n",
    "except (OSError, AttributeError):\n",
    "    libc = None\n",
    "\n",
    "__old_del = getattr(pd.DataFrame, '__del__', None)\n",
    "\n",
    "def __new_del(self):\n",
    "    if __old_del:\n",
    "        __old_del(self)\n",
    "    libc.malloc_trim(0)\n",
    "\n",
    "if libc:\n",
    "    print('Applying monkeypatch for pd.DataFrame.__del__', file=sys.stderr)\n",
    "    pd.DataFrame.__del__ = __new_del\n",
    "else:\n",
    "    print('Skipping monkeypatch for pd.DataFrame.__del__: libc or malloc_trim() not found', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8921483 rows of TRAIN and 7853253 rows of TEST\n"
     ]
    }
   ],
   "source": [
    "# SET THIS VARIABLE TO TRUE TO RUN KERNEL QUICKLY AND FIND BUGS\n",
    "# ONLY 10000 ROWS OF DATA IS LOADED\n",
    "Debug = False\n",
    "\n",
    "import numpy as np, pandas as pd, gc, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load(x):\n",
    "    ignore = ['MachineIdentifier']\n",
    "    if x in ignore: return False\n",
    "    else: return True\n",
    "\n",
    "# LOAD TRAIN AND TEST\n",
    "if Debug:\n",
    "    df_train = pd.read_csv('train.csv',dtype='category',usecols=load,nrows=10000)\n",
    "else:\n",
    "    df_train = pd.read_csv('train.csv',dtype='category',usecols=load)\n",
    "df_train['HasDetections'] = df_train['HasDetections'].astype('int8')\n",
    "if 5244810 in df_train.index:\n",
    "    df_train.loc[5244810,'AvSigVersion'] = '1.273.1144.0'\n",
    "    df_train['AvSigVersion'].cat.remove_categories('1.2&#x17;3.1144.0',inplace=True)\n",
    "\n",
    "if Debug:\n",
    "    df_test = pd.read_csv('test.csv',dtype='category',usecols=load,nrows=10000)\n",
    "else:\n",
    "    df_test = pd.read_csv('test.csv',dtype='category',usecols=load)\n",
    "    \n",
    "print('Loaded',len(df_train),'rows of TRAIN and',len(df_test),'rows of TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Encoding FunctionsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODE SEPARATELY\n",
    "def encode_FE(df,col):\n",
    "    vc = df[col].value_counts(dropna=False, normalize=True).to_dict()\n",
    "    nm = col+'_FE'\n",
    "    df[nm] = df[col].map(vc)\n",
    "    df[nm] = df[nm].astype('float32')\n",
    "    return [nm]\n",
    "\n",
    "# FREQUENCY ENCODE TOGETHER\n",
    "def encode_FE2(df1, df2, col):\n",
    "    df = pd.concat([df1[col],df2[col]])\n",
    "    vc = df.value_counts(dropna=False, normalize=True).to_dict()\n",
    "    nm = col+'_FE2'\n",
    "    df1[nm] = df1[col].map(vc)\n",
    "    df1[nm] = df1[nm].astype('float32')\n",
    "    df2[nm] = df2[col].map(vc)\n",
    "    df2[nm] = df2[nm].astype('float32')\n",
    "    return [nm]\n",
    "\n",
    "# FACTORIZE\n",
    "def factor_data(df_train, df_test, col):\n",
    "    df_comb = pd.concat([df_train[col],df_test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True)\n",
    "    # MAKE SMALLEST LABEL 1, RESERVE 0\n",
    "    df_comb += 1\n",
    "    # MAKE NAN LARGEST LABEL (need to remove attype('str') above)\n",
    "    df_comb = np.where(df_comb==0, df_comb.max()+1, df_comb)\n",
    "    df_train[col] = df_comb[:len(df_train)]\n",
    "    df_test[col] = df_comb[len(df_train):]\n",
    "    del df_comb\n",
    "    \n",
    "# OPTIMIZE MEMORY\n",
    "def reduce_memory(df,col):\n",
    "    mx = df[col].max()\n",
    "    if mx<256:\n",
    "            df[col] = df[col].astype('uint8')\n",
    "    elif mx<65536:\n",
    "        df[col] = df[col].astype('uint16')\n",
    "    else:\n",
    "        df[col] = df[col].astype('uint32')\n",
    "        \n",
    "# REDUCE CATEGORY CARDINALITY\n",
    "def relax_data(df_train, df_test, col):\n",
    "    cv1 = pd.DataFrame(df_train[col].value_counts().reset_index().rename({col:'train'},axis=1))\n",
    "    cv2 = pd.DataFrame(df_test[col].value_counts().reset_index().rename({col:'test'},axis=1))\n",
    "    cv3 = pd.merge(cv1,cv2,on='index',how='outer')\n",
    "    factor = len(df_test)/len(df_train)\n",
    "    cv3['train'].fillna(0,inplace=True)\n",
    "    cv3['test'].fillna(0,inplace=True)\n",
    "    cv3['remove'] = False\n",
    "    cv3['remove'] = cv3['remove'] | (cv3['train'] < len(df_train)/10000)\n",
    "    cv3['remove'] = cv3['remove'] | (factor*cv3['train'] < cv3['test']/3)\n",
    "    cv3['remove'] = cv3['remove'] | (factor*cv3['train'] > 3*cv3['test'])\n",
    "    cv3['new'] = cv3.apply(lambda x: x['index'] if x['remove']==False else 0,axis=1)\n",
    "    cv3['new'],_ = cv3['new'].factorize(sort=True)\n",
    "    cv3.set_index('index',inplace=True)\n",
    "    cc = cv3['new'].to_dict()\n",
    "    df_train[col] = df_train[col].map(cc)\n",
    "    reduce_memory(df_train,col)\n",
    "    df_test[col] = df_test[col].map(cc)\n",
    "    reduce_memory(df_test,col)\n",
    "    \n",
    "# DISPLAY MEMORY STATISTICS\n",
    "def display_memory(df_train, df_test):\n",
    "    print(len(df_train),'rows of training data use',df_train.memory_usage(deep=True).sum()//1e6,'Mb memory!')\n",
    "    print(len(df_test),'rows of test data use',df_test.memory_usage(deep=True).sum()//1e6,'Mb memory!')\n",
    "\n",
    "# CONVERT TO CATEGORIES\n",
    "def categorize(df_train, df_test, cols):\n",
    "    for col in cols:\n",
    "        df_train[col] = df_train[col].astype('category')\n",
    "        df_test[col] = df_test[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b3f6bdbb4e4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# AS timestamp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdatedictAS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AvSigVersionTimestamps.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DateAS'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AvSigVersion'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatedictAS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DateAS'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AvSigVersion'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatedictAS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    445\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[1;32m--> 447\u001b[1;33m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    448\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[1;31m# Try a pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[0;32m    697\u001b[0m                              \"allow_pickle=False\")\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "import gc\n",
    "\n",
    "# AS timestamp\n",
    "datedictAS = np.load('AvSigVersionTimestamps.npy')[()]\n",
    "df_train['DateAS'] = df_train['AvSigVersion'].map(datedictAS)\n",
    "df_test['DateAS'] = df_test['AvSigVersion'].map(datedictAS)\n",
    "\n",
    "# OS timestamp\n",
    "datedictOS = np.load('OSVersionTimestamps.npy')[()]\n",
    "df_train['DateOS'] = df_train['Census_OSVersion'].map(datedictOS)\n",
    "df_test['DateOS'] = df_test['Census_OSVersion'].map(datedictOS)\n",
    "\n",
    "# ENGINEERED FEATURE #1\n",
    "df_train['AppVersion2'] = df_train['AppVersion'].map(lambda x: np.int(x.split('.')[1]))\n",
    "df_test['AppVersion2'] = df_test['AppVersion'].map(lambda x: np.int(x.split('.')[1]))\n",
    "\n",
    "# ENGINEERED FEATURE #2\n",
    "df_train['Lag1'] = df_train['DateAS'] - df_train['DateOS']\n",
    "df_train['Lag1'] = df_train['Lag1'].map(lambda x: x.days//7)\n",
    "df_test['Lag1'] = df_test['DateAS'] - df_test['DateOS']\n",
    "df_test['Lag1'] = df_test['Lag1'].map(lambda x: x.days//7)\n",
    "\n",
    "# ENGINEERED FEATURE #3\n",
    "df_train['Lag5'] = datetime(2018,7,26) - df_train['DateAS']\n",
    "df_train['Lag5'] = df_train['Lag5'].map(lambda x: x.days//1)\n",
    "df_train.loc[ df_train['Lag5']<0, 'Lag5' ] = 0\n",
    "df_test['Lag5'] = datetime(2018,9,27) - df_test['DateAS'] #PUBLIC TEST\n",
    "df_test['Lag5'] = df_test['Lag5'].map(lambda x: x.days//1)\n",
    "df_test.loc[ df_test['Lag5']<0, 'Lag5' ] = 0\n",
    "df_train['Lag5'] = df_train['Lag5'].astype('float32') # allow for NAN\n",
    "df_test['Lag5'] = df_test['Lag5'].astype('float32') # allow for NAN\n",
    "\n",
    "# ENGINEERED FEATURE #4\n",
    "df_train['driveA'] = df_train['Census_SystemVolumeTotalCapacity'].astype('float')/df_train['Census_PrimaryDiskTotalCapacity'].astype('float')\n",
    "df_test['driveA'] = df_test['Census_SystemVolumeTotalCapacity'].astype('float')/df_test['Census_PrimaryDiskTotalCapacity'].astype('float')\n",
    "df_train['driveA'] = df_train['driveA'].astype('float32') \n",
    "df_test['driveA'] = df_test['driveA'].astype('float32') \n",
    "\n",
    "# ENGINNERED FEATURE #5\n",
    "df_train['driveB'] = df_train['Census_PrimaryDiskTotalCapacity'].astype('float') - df_train['Census_SystemVolumeTotalCapacity'].astype('float')\n",
    "df_test['driveB'] = df_test['Census_PrimaryDiskTotalCapacity'].astype('float') - df_test['Census_SystemVolumeTotalCapacity'].astype('float')\n",
    "df_train['driveB'] = df_train['driveB'].astype('float32') \n",
    "df_test['driveB'] = df_test['driveB'].astype('float32') \n",
    "\n",
    "cols6=['Lag1']\n",
    "cols8=['Lag5','driveB','driveA']\n",
    "\n",
    "del df_train['DateAS'], df_train['DateOS'] #, df_train['DateBL']\n",
    "del df_test['DateAS'], df_test['DateOS'] #, df_test['DateBL']\n",
    "del datedictAS, datedictOS\n",
    "x=gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Lag1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\kiit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Lag1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e198c387e3f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mFE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Census_OSVersion'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Census_OSBuildRevision'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Census_InternalBatteryNumberOfCharges'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'AvSigVersion'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Lag1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mFE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mcols3\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mencode_FE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mencode_FE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-29db6c819abb>\u001b[0m in \u001b[0;36mencode_FE\u001b[1;34m(df, col)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# FREQUENCY ENCODE SEPARATELY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mencode_FE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mnm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_FE'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kiit\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Lag1'"
     ]
    }
   ],
   "source": [
    "cols3 = []\n",
    "# ENGINEERED FEATURES #6, #7, #8, #9, #10\n",
    "FE = ['Census_OSVersion', 'Census_OSBuildRevision', 'Census_InternalBatteryNumberOfCharges', 'AvSigVersion', 'Lag1']\n",
    "for col in FE:\n",
    "    cols3 += encode_FE(df_train, col)\n",
    "    encode_FE(df_test, col)\n",
    "    \n",
    "# ENGINEERED FEATURES #11, #12\n",
    "FE2 = ['CountryIdentifier', 'Census_InternalBatteryNumberOfCharges']\n",
    "for col in FE2:\n",
    "    cols3 += encode_FE2(df_train, df_test, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical versus Numerical Encod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CE = ['CountryIdentifier', 'SkuEdition', 'Firewall', 'Census_ProcessorCoreCount', 'Census_OSUILocaleIdentifier', 'Census_FlightRing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [x for x in df_train.columns if x not in ['HasDetections']+CE+cols3+cols6+cols8]\n",
    "cols2 = CE; ct = 1\n",
    "    \n",
    "for col in cols.copy():\n",
    "    rate = df_train[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "    if rate > 0.98:\n",
    "        del df_train[col]\n",
    "        del df_test[col]\n",
    "        cols.remove(col)\n",
    "        ct += 1\n",
    "\n",
    "rmv3=['Census_OSSkuName', 'OsVer', 'Census_OSArchitecture', 'Census_OSInstallLanguageIdentifier']\n",
    "rmv4=['SMode']\n",
    "for col in rmv3+rmv4:\n",
    "    del df_train[col]\n",
    "    del df_test[col]\n",
    "    cols.remove(col)\n",
    "    ct +=1\n",
    "    \n",
    "print('Removed',ct,'variables')\n",
    "x=gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Feature Cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Factorizing...')\n",
    "for col in cols+cols2+cols6:\n",
    "    factor_data(df_train, df_test, col)\n",
    "print('Relaxing data...')\n",
    "for col in cols+cols2: relax_data(df_train, df_test, col)\n",
    "print('Optimizing memory...')\n",
    "for col in cols+cols2+cols6:\n",
    "    reduce_memory(df_train, col)\n",
    "    reduce_memory(df_test, col)\n",
    "# Converting 6 variables to categorical\n",
    "categorize(df_train, df_test, cols2)\n",
    "    \n",
    "print('Number of variables is',len(cols+cols2+cols3+cols6+cols8))\n",
    "display_memory(df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pred_val = np.zeros(len(df_test))\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "ct = 0\n",
    "for idxT, idxV in folds.split(df_train[cols+cols2+cols3+cols6], df_train['HasDetections']):\n",
    "    # TRAIN LGBM\n",
    "    ct += 1; print('####### FOLD ',ct,'#########')\n",
    "    df_trainA = df_train.loc[idxT]\n",
    "    df_trainB = df_train.loc[idxV]\n",
    "    model = lgb.LGBMClassifier(n_estimators=10000, colsample_bytree=0.5, objective='binary', num_leaves=2048,\n",
    "            max_depth=-1, learning_rate=0.04)\n",
    "    h=model.fit(df_trainA[cols+cols2+cols3+cols6+cols8], df_trainA['HasDetections'], eval_metric='auc',\n",
    "            eval_set=[(df_trainB[cols+cols2+cols3+cols6+cols8], df_trainB['HasDetections'])], verbose=200,\n",
    "            early_stopping_rounds=100)\n",
    "    \n",
    "    # PREDICT TEST\n",
    "    del df_trainA, df_trainB; x=gc.collect()\n",
    "    idx = 0; ct2 = 1; chunk = 1000000\n",
    "    print('Predicting test...')\n",
    "    while idx < len(df_test):\n",
    "        idx2 = min(idx + chunk, len(df_test) )\n",
    "        idx = range(idx, idx2)\n",
    "        pred_val[idx] += model.predict_proba(df_test.iloc[idx][cols+cols2+cols3+cols6+cols8])[:,1]\n",
    "        #print('Finished predicting part',ct2)\n",
    "        ct2 += 1; idx = idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train; x=gc.collect()\n",
    "df_test = pd.read_csv('../input/microsoft-malware-prediction/test.csv',\n",
    "            usecols=['MachineIdentifier','AvSigVersion'], nrows=len(pred_val))\n",
    "\n",
    "# CORRECT PREDICTIONS FOR OUTLIERS IN PRIVATE TEST DATASET\n",
    "from datetime import datetime\n",
    "datedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy')[()]\n",
    "df_test['Date'] = df_test['AvSigVersion'].map(datedictAS)\n",
    "df_test['HasDetections'] = pred_val / 5.0\n",
    "df_test['X'] = df_test['Date'] - datetime(2018,11,20,4,0) \n",
    "df_test['X'] = df_test['X'].map(lambda x: x.total_seconds()/86400)\n",
    "df_test['X'].fillna(0,inplace=True)\n",
    "s = 5.813888\n",
    "df_test['F'] = 1.0\n",
    "df_test['F'] = 1 - df_test['X']/s\n",
    "df_test.loc[df_test['X']<=0,'F'] = 1.0\n",
    "df_test.loc[df_test['X']>s,'F'] = 0\n",
    "df_test['HasDetections'] *= df_test['F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['MachineIdentifier','HasDetections']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt    \n",
    "b = plt.hist(df_test['HasDetections'], bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['ones'] = 1\n",
    "dynamicPlot(df_test, 'ones', inc_dy=2, legend=0,\n",
    "        title='Test Predictions. (Dotted line uses right y-axis. Solid uses left.)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
